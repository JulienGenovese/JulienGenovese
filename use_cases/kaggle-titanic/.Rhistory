# loading libraries
library(dplyr)
library(randomForest)
library(forecast)
library(ggplot2)
library(stringr)
###load the dataset  ----
cat("\014")
rm(list = ls())
trainSet<-read.csv2("./../input/train.csv",sep =",",stringsAsFactors = FALSE)
testSet<-read.csv2("./../input/test.csv",sep=",")
colnames(trainSet)
###### feature engineering ----
learn3<-trainSet %>% select(c("Survived","Embarked")) %>% group_by(Embarked)
summarise(learn3, sumSurvived = sum(Survived, na.rm = TRUE)/n())
# we see that the embarked port seeems not to be very important to survive.
# it makes sense
learn4<-trainSet %>% select(c("Survived","Pclass")) %>% group_by(Pclass)
summarise(learn4, sumSurvived = sum(Survived, na.rm = TRUE)/n())
# on the contrary the Pclass seems to bee important at first sight
t1<-trainSet %>% mutate(isCabin = !(Cabin == "")) %>% group_by(isCabin)
summarise(t1, sumSurvived = sum(Survived, na.rm = TRUE)/n())
# for the moment also isCabin seems to be important
trainSet$Title<-unlist(lapply(strsplit(trainSet$Name,split=",") ,FUN=  function(name) {
str_trim(strsplit(name[2],split="\\.")[[1]][1])
}
))
# loading libraries
library(dplyr)
library(randomForest)
library(forecast)
library(ggplot2)
library(stringr)
###load the dataset  ----
cat("\014")
rm(list = ls())
trainSet<-read.csv2("./../input/train.csv",sep =",",stringsAsFactors = FALSE)
testSet<-read.csv2("./../input/test.csv",sep=",")
colnames(trainSet)
getwd()
setwd("..")
list.files()
setwd("./kaggle-titanic")
# loading libraries
library(dplyr)
library(randomForest)
library(forecast)
library(ggplot2)
library(stringr)
###load the dataset  ----
cat("\014")
rm(list = ls())
trainSet<-read.csv2("./../input/train.csv",sep =",",stringsAsFactors = FALSE)
testSet<-read.csv2("./../input/test.csv",sep=",")
colnames(trainSet)
trainSet<-read.csv2("./input/train.csv",sep =",",stringsAsFactors = FALSE)
testSet<-read.csv2("./input/test.csv",sep=",")
colnames(trainSet)
learn3<-trainSet %>% select(c("Survived","Embarked")) %>% group_by(Embarked)
trainSet$Title<-unlist(lapply(strsplit(trainSet$Name,split=",") ,FUN=  function(name) {
str_trim(strsplit(name[2],split="\\.")[[1]][1])
}
))
plot(trainSet$Title)
plot(trainSet$Title %<>% as.factor)
library(magrittr)
plot(trainSet$Title %<>% as.factor)
# loading libraries
library(dplyr)
library(randomForest)
library(forecast)
library(ggplot2)
library(stringr)
###load the dataset  ----
cat("\014")
rm(list = ls())
trainSet<-read.csv2("./input/train.csv",sep =",",stringsAsFactors = FALSE)
testSet<-read.csv2("./input/test.csv",sep=",")
colnames(trainSet)
###### feature engineering ----
learn3<-trainSet %>% select(c("Survived","Embarked")) %>% group_by(Embarked)
summarise(learn3, sumSurvived = sum(Survived, na.rm = TRUE)/n())
# we see that the embarked port seeems not to be very important to survive.
# it makes sense
learn4<-trainSet %>% select(c("Survived","Pclass")) %>% group_by(Pclass)
summarise(learn4, sumSurvived = sum(Survived, na.rm = TRUE)/n())
# on the contrary the Pclass seems to bee important at first sight
t1<-trainSet %>% mutate(isCabin = !(Cabin == "")) %>% group_by(isCabin)
summarise(t1, sumSurvived = sum(Survived, na.rm = TRUE)/n())
# for the moment also isCabin seems to be important
trainSet$Title<-unlist(lapply(strsplit(trainSet$Name,split=",") ,FUN=  function(name) {
str_trim(strsplit(name[2],split="\\.")[[1]][1])
}
))
# try to work only with a part of the dataset
X_train<-trainSet %>% mutate(isCabin = !(Cabin == "")) %>%
select(-c("PassengerId","Name","Ticket","Cabin"))
# converting data type ----
X_train$Sex<- X_train$Sex %>% as.factor
X_train$Pclass<- X_train$Pclass %>% as.factor
X_train$Embarked<- X_train$Embarked %>% as.factor
X_train$Age<-X_train$Age %>% as.numeric
X_train$Survived <- X_train$Survived %>% as.factor
X_train$Fare<-X_train$Fare %>% as.numeric
X_train$Title<-X_train$Title %>% as.factor
###
summary(X_train)
colSums(is.na(X_train))
# percentage of na in Age and Embarked
trainGrouped <- X_train %>% group_by(Sex,Pclass)
summarise(trainGrouped, meanAge = mean(Age, na.rm = TRUE) , meadianAge = median(Age, na.rm = TRUE),numberOfNA_Age = sum(is.na(Age))/nrow(X_train),)
sum(is.na(X_train$Age))/sum(nrow(X_train))
sum(is.na(X_train$Embarked))/sum(nrow(X_train))
learning<-X_train[1:600,]
testing<-X_train[600:891,]
##### SEE DISTRIBUTION AGE
histAgeBeforeFill<-ggplot(X_train[!is.na(X_train$Age),], aes(x=Age)) +
geom_histogram(aes(y=..density..), colour="black", fill="white")+
geom_density(alpha=.2, fill="#FF6666")
histAgeBeforeFill
methodFillNA_Age<-"mean"
summaryBefFill<-summary(X_train$Age)
qplot(Pclass,Age, data=X_train, geom=c("boxplot"),
fill=Pclass, main="Age by Category",
xlab="", ylab="Age")
qplot(Pclass,Age, data=X_train, geom=c("boxplot"),
fill=Pclass, main="Age by Category",
xlab="", ylab="Age")
qplot(Embarked,Age, data=X_train, geom=c("boxplot"),
fill=Embarked, main="Age by Category",
xlab="", ylab="Age")
