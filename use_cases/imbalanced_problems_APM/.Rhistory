ncol(trainXnnet)
10 * (ncol(trainXnnet) + 1) + 10 + 1
3 * (ncol(trainXnnet) + 1) + 3 + 1
set.seed(100)
print(nnetTune)
plot(nntetTune)
plot(nnetTune)
enetTune
print(nnetTune)
library(doMC)
registerDoMC(cores = 5)
tooHigh <- findCorrelation(cor(solTrainXtrans), cutoff = .75)
trainXnnet <- solTrainXtrans[,-tooHigh]
testXnnet <- solTestXtrans[,-tooHigh]
nnetGrid <- expand.grid(.decay = c(0.01, .1), # regularization parameter
.size = c(1:10), # size for gradient descent
.bag = FALSE) # use bagging instead of different random seeds
set.seed(100)
nnetTune <- train(
solTrainXtrans,
solTrainY,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = ctrl,
preProc = c("center", "scale"), # center and scale
linout = TRUE, #
trace = FALSE, # less output printed
MaxNWts = 5 * (ncol(trainXnnet) + 1) + 3 + 1,
maxit = 500
)
print(nnetTune)
View(nnetTune)
plot(nnetTune)
set.seed(100)
nnetTune <- train(
solTrainXtrans,
solTrainY,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = ctrl,
preProc = c("center", "scale"), # center and scale
linout = TRUE, #
trace = FALSE, # less output printed
MaxNWts = 3 * (ncol(trainXnnet) + 1) + 3 + 1,
maxit = 500
)
print(nnetTune)
plot(nnetTune)
library(doMC)
registerDoMC(cores = 3)
tooHigh <- findCorrelation(cor(solTrainXtrans), cutoff = .75)
trainXnnet <- solTrainXtrans[,-tooHigh]
testXnnet <- solTestXtrans[,-tooHigh]
nnetGrid <- expand.grid(.decay = c(0.01, .1), # regularization parameter
.size = c(1:10), # size for gradient descent
.bag = FALSE) # use bagging instead of different random seeds
colSums(trainXnnet)
colSums(is.na(trainXnnet))
library(AppliedPredictiveModeling)
data(solubility)
ls(pattern = "^solT")
library(doMC)
registerDoMC(cores = 3)
tooHigh <- findCorrelation(cor(solTrainXtrans), cutoff = .75)
trainXnnet <- solTrainXtrans[,-tooHigh]
testXnnet <- solTestXtrans[,-tooHigh]
nnetGrid <- expand.grid(.decay = c(0.01, .1), # regularization parameter
.size = c(1:10), # size for gradient descent
.bag = FALSE) # use bagging instead of different random seeds
set.seed(100)
library(doMC)
registerDoMC(cores = 3)
tooHigh <- findCorrelation(cor(solTrainXtrans), cutoff = .75)
trainXnnet <- solTrainXtrans[,-tooHigh]
testXnnet <- solTestXtrans[,-tooHigh]
nnetGrid <- expand.grid(.decay = c(0.01, .1), # regularization parameter
.size = c(1:10), # size for gradient descent
.bag = FALSE) # use bagging instead of different random seeds
set.seed(100)
nnetTune <- train(
trainXnnet,
solTrainY,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = ctrl,
preProc = c("center", "scale"), # center and scale
linout = TRUE, # we want a regression model
trace = FALSE, # less output printed
MaxNWts = 3 * (ncol(trainXnnet) + 1) + 3 + 1, # 3 hidden layers
maxit = 500
)
library(doMC)
registerDoMC(cores = 3)
tooHigh <- findCorrelation(cor(solTrainXtrans), cutoff = .75)
trainXnnet <- solTrainXtrans[,-tooHigh]
testXnnet <- solTestXtrans[,-tooHigh]
nnetGrid <- expand.grid(.decay = c(0.01, .1), # regularization parameter
.size = c(1:10), # size for gradient descent
.bag = FALSE) # use bagging for resampling
ctrl <- trainControl(method = "cv", number = 10)
set.seed(100)
nnetTune <- train(
trainXnnet,
solTrainY,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = ctrl,
preProc = c("center", "scale"), # center and scale
linout = TRUE, # we want a regression model
trace = FALSE, # less output printed
MaxNWts = 3 * (ncol(trainXnnet) + 1) + 3 + 1, # 3 hidden layers
maxit = 500
)
nnetTune
nnetTune <- train(
trainXnnet,
solTrainY,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = ctrl,
preProc = c("center", "scale", "pca"), # center and scale
linout = TRUE, # we want a regression model
trace = FALSE, # less output printed
MaxNWts = 3 * (ncol(trainXnnet) + 1) + 3 + 1, # 3 hidden layers
maxit = 500
)
print(nnetTune)
plot(nnetTune)
library(doMC)
registerDoMC(cores = 3)
tooHigh <- findCorrelation(cor(solTrainXtrans), cutoff = .75)
trainXnnet <- solTrainXtrans[,-tooHigh]
testXnnet <- solTestXtrans[,-tooHigh]
nnetGrid <- expand.grid(.decay = c(0.01, .1), # regularization parameter
.size = c(1:10), # size for gradient descent
.bag = FALSE) # use bagging for resampling
ctrl <- trainControl(method = "cv", number = 10)
set.seed(100)
nnetTune <- train(
trainXnnet,
solTrainY,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = ctrl,
preProc = c("center", "scale", "pca"), # center and scale
linout = TRUE, # we want a regression model
trace = FALSE, # less output printed
MaxNWts = 6 * (ncol(trainXnnet) + 1) + 6 + 1, # 3 hidden layers
maxit = 500
)
plot(nnetTune)
print(nnetTune)
plot(nnetTune)
library(caret)
library(doMC)
rm(list = ls())
cat("\014")
data(tecator)
?tecator
pcaObject <- prcomp(absorp,
center = TRUE,
scale. = TRUE
)
pcaObject <- prcomp(absorp,
center = TRUE,
scale. = TRUE
)
percentVariance <- pcaObject$sd^2/sum(pcaObject$sd^2)*100
percentVariance
plot(percentVariance)
varexp <- cumsum(percentVariance)
which(varexp >= 99.999999)
percentVariance
percentVariance[ยน_5]
percentVariance[1:5]
absorp
endpoints
data <- data.frame(endpoints[,2], absorp)
names(data)[1] <- "fat"
nzr <- nearZeroVar(data, saveMetrics = TRUE)
hist(data$fat)
nzr
trainingRows <- createDataPartition(data$fat, p = .8, list = FALSE)
trainingData <- data[trainingRows, ]
testData <- data[-trainingRows,]
registerDoMC(cores = 5)
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(101)
lmFit <- train(fat ~ .,
data = trainingData,
method = "lm",
trControl = ctrl)
set.seed(101)
lmFitPCA <- train(
fat ~ .,
data = trainingData,
method = "pcr",
preProcess = c("center", "scale"),
trControl = ctrl,
tuneLength = 100
)
set.seed(101)
rlmPCA <- train(fat ~ .,
data = trainingData,
method = "rlm",
preProcess = "pca",
trControl = ctrl
)
set.seed(101)
plsTune <- train(fat ~ .,
data = trainingData,
method = "pls",
tuneLength = 200,
trControl = ctrl,
preProc = c("center", "scale")
)
enetGrid <- expand.grid(.lambda = c(0, 0.01, .1),
.fraction = seq(.05, 1, length = 20)
)
set.seed(101)
enetTune <- train(fat ~ .,
data = trainingData,
method = "enet",
tuneGrid = enetGrid,
trControl = ctrl,
preProc = c("center", "scale")
)
resamp <- resamples(list(linear = lmFit,
linearPCA = lmFitPCA,
linearePLS = plsTune,
linearRobust = rlmPCA,
elasticNet = enetTune))
ctrl
nnetGrid <- expand.grid(.decay = c(0., 0.01, .1), # regularization parameter
.size = c(1:10), # size for gradient descent
.bag = FALSE) # use bagging for resampling
set.seed(101)
nnetTune <- train(
fat ~ .,
data = trainingData,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = ctrl,
preProc = c("center", "scale"), # center and scale
linout = TRUE, # we want a regression model
trace = FALSE, # less output printed
MaxNWts = 4 * (ncol(trainXnnet) + 1) + 4 + 1, # 3 hidden layers
maxit = 500
)
nnetTune
nnetTune <- train(
fat ~ .,
data = trainingData,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = ctrl,
preProc = c("center", "scale"), # center and scale
linout = TRUE, # we want a regression model
trace = FALSE, # less output printed
MaxNWts = 4 * (ncol(trainingData) + 1) + 4 + 1, # 3 hidden layers
maxit = 500
)
resamp <- resamples(list(linear = lmFit,
linearPCA = lmFitPCA,
linearePLS = plsTune,
linearRobust = rlmPCA,
elasticNet = enetTune,
neuralNet = nnetTune))
bwplot(resamp)
nnetTune
plsTune
resamp <- resamples(list(
linearePLS = plsTune,
elasticNet = enetTune,
neuralNet = nnetTune))
bwplot(resamp)
plot(nnetTune)
resamp <- resamples(list(
linearePLS = plsTune,
elasticNet = enetTune,
neuralNet = nnetTune))
bwplot(resamp)
.rs.restartR()
library(doMC)
rm(list = ls())
library(caret)
library(doMC)
rm(list = ls())
cat("\014")
library(AppliedPredictiveModeling)
data(permeability)
dim(fingerprints)
nzr <- nearZeroVar(fingerprints)
fingerprintsNoVar <- fingerprints[,-nzr]
dim(fingerprintsNoVar)
data <- data.frame(permeability,fingerprintsNoVar)
colnames(data)[1] <- "permeability"
transBox <- BoxCoxTrans(data$permeability)
trans <- predict(transBox, data$permeability)
hist(trans)
data$permeability <- trans
library(caret)
library(doMC)
rm(list = ls())
cat("\014")
library(AppliedPredictiveModeling)
data(permeability)
dim(fingerprints)
nzr <- nearZeroVar(fingerprints)
fingerprintsNoVar <- fingerprints[,-nzr]
dim(fingerprintsNoVar)
data <- data.frame(permeability,fingerprintsNoVar)
colnames(data)[1] <- "permeability"
transBox <- BoxCoxTrans(data$permeability)
trans <- predict(transBox, data$permeability)
hist(trans)
data$permeability <- trans
trainingRows <- createDataPartition(permeability, p = .8, list = FALSE)
trainingData <- data[trainingRows, ]
testData <- data[-trainingRows,]
registerDoMC(cores = 5)
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(101)
plsTune <- train(permeability~ .,
data = trainingData,
method = "pls",
tuneLength = 50,
trControl = ctrl,
preProc = c("center", "scale")
)
plot(plsTune)
prediction <- predict(plsTune, testData[,-1])
lmPls <- data.frame(obs = testData$permeability, pred = prediction)
defaultSummary(lmPls)
axisRange <- extendrange(c(lmPls$obs,lmPls$pred))
plot(lmPls$obs, lmPls$pred,
ylim = axisRange,
xlim = axisRange)
abline(0, 1, col = "darkgrey", lty = 2)
defaultSummary(lmPls)
set.seed(101)
lmFitPCA <- train(
permeability~ .,
data = trainingData,
method = "pcr",
preProcess = c("center", "scale"),
trControl = ctrl,
tuneLength = 100
)
set.seed(101)
rlmPCA <- train(permeability~ .,
data = trainingData,
method = "rlm",
preProcess = "pca",
trControl = ctrl
)
enetGrid <- expand.grid(.lambda = c(0, 0.01, .1),
.fraction = seq(.05, 1, length = 20)
)
set.seed(101)
enetTune <- train(permeability~ .,
data = trainingData,
method = "enet",
tuneGrid = enetGrid,
trControl = ctrl,
preProc = c("center", "scale")
)
resamp <- resamples(list(linearPCA = lmFitPCA,
linearePLS = plsTune,
linearRobust = rlmPCA,
elasticNet = enetTune))
bwplot(resamp)
set.seed(101)
nnetTune <- train(
permeability~ .,
data = trainingData,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = ctrl,
preProc = c("center", "scale"), # center and scale
linout = TRUE, # we want a regression model
trace = FALSE, # less output printed
MaxNWts = 8 * (ncol(trainingData) + 1) + 8 + 1, # 3 hidden layers
maxit = 500
)
nnetGrid <- expand.grid(.decay = c(0., 0.01, .1), # regularization parameter
.size = c(1:10), # size for gradient descent
.bag = FALSE) # use bagging for resampling
enetTune <- train(permeability~ .,
data = trainingData,
method = "enet",
tuneGrid = enetGrid,
trControl = ctrl,
preProc = c("center", "scale")
)
set.seed(101)
nnetGrid <- expand.grid(.decay = c(0., 0.01, .1), # regularization parameter
.size = c(1:10), # size for gradient descent
.bag = FALSE) # use bagging for resampling
nnetTune <- train(
permeability~ .,
data = trainingData,
method = "avNNet",
tuneGrid = nnetGrid,
trControl = ctrl,
preProc = c("center", "scale"), # center and scale
linout = TRUE, # we want a regression model
trace = FALSE, # less output printed
MaxNWts = 8 * (ncol(trainingData) + 1) + 8 + 1, # 3 hidden layers
maxit = 500
)
install.packages(c('rzmq','repr','IRkernel','IRdisplay'),
repos = c('http://irkernel.github.io/',
getOption('repos')),
type = 'source')
IRkernel::installspec(user = FALSE)
# loading libraries
library(dplyr)
library(randomForest)
library(forecast)
library(ggplot2)
library(stringr)
###load the dataset  ----
cat("\014")
rm(list = ls())
trainSet<-read.csv2("./../input/train.csv",sep =",",stringsAsFactors = FALSE)
testSet<-read.csv2("./../input/test.csv",sep=",")
colnames(trainSet)
# loading libraries
library(dplyr)
library(randomForest)
library(forecast)
library(ggplot2)
library(stringr)
###load the dataset  ----
cat("\014")
rm(list = ls())
trainSet<-read.csv2("./input/train.csv",sep =",",stringsAsFactors = FALSE)
testSet<-read.csv2("./input/test.csv",sep=",")
setwd()
getwd()
list.files()
set.seed(1)
dat <- twoClassSim(2000)
library(AppliedPredictiveModeling)
set.seed(1)
dat <- twoClassSim(2000)
library(caret)
set.seed(1)
dat <- twoClassSim(2000)
set.seed(2)
mod <- train(Class ~ .,
data = dat,
method = "lda",
trControl = trainControl(savePredictions = TRUE, classProbs = TRUE))
View(dat)
str(dat)
cal <- calibration(obs ~ Class1, data = mod$pred)
xyplot(cal)
View(mod)
78/26
78/13
3/4
0.25*2
0.5*0.5
1/4+1&/2
1/4+1/2
1/2+3/4
1/4+1/2+3/4
1/4+1/2+3/4+3
1642.85*14
30000/13
library(tidyverse)
library(magrittr)
library(caret)
library(arules)
library(doMC)
library(pROC)
library(corrplot)
setwd("/home/julien/My_Programs/imbalanced_problems_APM")
trainingFilled <- readRDS("./dataset/trainingFilled.RDS")
evaluationFilled <- readRDS("./dataset/evaluationFilled.RDS")
testingFilled <- readRDS("./dataset/testingFilled.RDS")
fiveStats <- function(...) c(twoClassSummary(...),
defaultSummary(...))
fourStats <- function(data, lev = levels(data$obs), model = NULL){
accKappa <- postResample(data[, "pred"], data[, "obs"])
out <- c(accKapp,
sensitivity(data[, "pred"], data[,"obs"],lev[1]),
specificity(data[, "pred"], data[,"obs"],lev[2]))
names(out)[3:4] <- c("Sens", "Spec")
out
}
ctrl <- trainControl(method = "cv",
classProbs = TRUE,
summaryFunction = fiveStats,
verboseIter = TRUE,
savePred = TRUE
)
ctrlNoprob <- ctrl
ctrlNoprob$summaryFunction <- fourStats
ctrlNoprob$classProbs <- FALSE
ctrl$sampling <- "smote"
set.seed(2000)
registerDoMC(cores = 5)
rfSMOTE <- train(income.L ~ ., data = trainingFilled,
method = "rf",
trControl = ctrl,
ntree = 1500,
tuneLength = 10,
metric = "ROC")
saveRDS(rfSMOTE, "./models/rfSMOTE.RDS")
