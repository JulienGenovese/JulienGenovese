geom_line(aes(iter, train_rmse_mean), color = "red") +
geom_line(aes(iter, test_rmse_mean), color = "blue")
########### XGBOOST BY http://uc-r.github.io/gbm_regression ########
library(rsample)      # data splitting
library(xgboost)      # a faster implementation of gbm
library(caret)        # an aggregator package for performing many machine learning models
library(pdp)          # model visualization
library(ggplot2)      # model visualization
library(lime)         # model visualization
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
cat("\014")
rm(list = ls())
set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
features <- setdiff(names(ames_train), "Sale_Price")
# Create the treatment plan from the training data
treatplan <- vtreat::designTreatmentsZ(ames_train, features, verbose = FALSE)
# Get the "clean" variable names from the scoreFrame
new_vars <- treatplan %>%
magrittr::use_series(scoreFrame) %>%
dplyr::filter(code %in% c("clean", "lev")) %>%
magrittr::use_series(varName)
View(treatplan)
treatplan[["vtreatVersion"]][[1]]
# Prepare the training data
features_train <- vtreat::prepare(treatplan, ames_train, varRestriction = new_vars) %>% as.matrix()
response_train <- ames_train$Sale_Price
# Prepare the test data
features_test <- vtreat::prepare(treatplan, ames_test, varRestriction = new_vars) %>% as.matrix()
response_test <- ames_test$Sale_Price
# dimensions of one-hot encoded data
dim(features_train)
## [1] 2051  208
dim(features_test)
## [1] 879 208
# reproducibility
set.seed(123)
xgb.fit1 <- xgb.cv(
data = features_train,
label = response_train,
nrounds = 1000, # number of trees
nfold = 5, # k fold to evaluate training and test errors
objective = "reg:linear",  # for regression models
verbose = 0               # silent,
)
library(MASS)
library(ISLR)
fix(Boston)
names(Boston)
lm.fit=lm(medv~lstat)
lm.fit=lm(medv~lstat,data=Boston)
plot(medv~lstat,data=Boston)
lm.fit=lm(medv~lstat,data=Boston)
attach(Boston)
lm.fit=lm(medv~lstat)
lm.fit
summary(lm.fit)
attach(Boston)
lm.fit=lm(medv~lstat)
lm.fit
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
confint(lm.fit)
predict(lm.fit,data.frame(lstat=(c(5,10,15))), interval="confidence")
predict(lm.fit,data.frame(lstat=(c(5,10,15))), interval="prediction")
plot(lstat,medv)
abline(lm.fit)
abline(lm.fit,lwd=3)
abline(lm.fit,lwd=3,col="red")
plot(lstat,medv,col="red")
plot(lstat,medv,pch=20)
plot(lstat,medv,pch="+")
plot(1:20,1:20,pch=1:20)
plot(lstat,medv,pch=20)
plot(1:20,1:20,pch=1:20)
confint(lm.fit)
predict(lm.fit,data.frame(lstat=(c(5,10,15))), interval="confidence")
predict(lm.fit,data.frame(lstat=(c(5,10,15))), interval="prediction")
plot(lm.fit)
lm.fit
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
plot(lstat,medv)
abline(lm.fit,lwd=3,col="red")
plot(lstat,medv,col="red")
plot(lstat,medv,pch=20)
confint(lm.fit)
predict(lm.fit,data.frame(lstat=(c(5,10,15))), interval="confidence")
plot(lm.fit)
# Simple Linear Regression
fix(Boston)
names(Boston)
plot(medv~lstat,data=Boston)
lm.fit=lm(medv~lstat,data=Boston)
?Boston # to understand the dataset
attach(Boston)
lm.fit=lm(medv~lstat)
lm.fit
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
plot(lstat,medv)
abline(lm.fit,lwd=3,col="red")
confint(lm.fit)
predict(lm.fit,data.frame(lstat=(c(5,10,15))), interval="confidence")
plot(lm.fit)
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
plot(lm.fit)
plot(lm.fit)
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
plot(hatvalues(lm.fit))
lm.fit=lm(medv~lstat+age,data=Boston)
lm.fit=lm(medv~lstat+age,data=Boston)
summary(lm.fit)
lm.fit=lm(medv~.,data=Boston)
summary(lm.fit)
plot(lm.fit)
par(mfrow=c(2,2))
plot(lm.fit)
par(mfrow=c(2,2))
plot(lm.fit)
lm.fit1=lm(medv~.-age-indus,data=Boston)
summary(lm.fit1)
lm.fit1=update(lm.fit, ~.-age)
summary(lm(medv~lstat*age,data=Boston))
summary(lm(medv~lstat*age,data=Boston))
lm.fit2=lm(medv~lstat+I(lstat^2))
summary(lm.fit2)
fit5 = lm(medv~lstat*age,data=Boston)
fit6=lm(medv~lstat+I(lstat^2))
summary(fit5)
# Non-linear Transformations of the Predictors
fit6=lm(medv~lstat+I(lstat^2))
summary(lm.fit6)
summary(fit6)
par(mfrow =c(1,1))
plot(medv~lstat)
points(lstat,fitted(fit6), col= "red", pch= 20)
lm.fit5=lm(medv~poly(lstat,4))
summary(lm.fit5)
fit7=lm(medv~poly(lstat,4))
points(lstat,fitted(fit7), col= "blue", pch= 20)
fix(Boston)
names(Boston)
plot(medv~lstat,data=Boston)
lm.fit=lm(medv~lstat,data=Boston)
?Boston # to understand the dataset
attach(Boston)
lm.fit=lm(medv~lstat)
lm.fit
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
plot(lstat,medv)
abline(lm.fit,lwd=3,col="red")
confint(lm.fit)
predict(lm.fit,data.frame(lstat=(c(5,10,15))), interval="confidence")
lm.fit=lm(medv~lstat+age,data=Boston)
# Chapter 3 Lab: Linear Regression
library(MASS) # some datasets we want to access
library(ISLR) # some other datasets
# Simple Linear Regression
fix(Boston)
names(Boston)
plot(medv~lstat,data=Boston)
lm.fit=lm(medv~lstat,data=Boston)
?Boston # to understand the dataset
attach(Boston)
fit1=lm(medv~lstat)
fit1
summary(fit1)
names(fit1)
coef(fit1)
plot(lstat,medv)
abline(fit1,lwd=3,col="red")
confint(fit1)
predict(fit1,data.frame(lstat=(c(5,10,15))), interval="confidence")
# Chapter 3 Lab: Linear Regression
library(MASS) # some datasets we want to access
library(ISLR) # some other datasets
# Simple Linear Regression
fix(Boston)
names(Boston)
plot(medv~lstat,data=Boston)
lm.fit=lm(medv~lstat,data=Boston)
?Boston # to understand the dataset
attach(Boston)
fit1=lm(medv~lstat)
fit1
summary(fit1)
names(fit1)
coef(fit1)
plot(lstat,medv)
abline(fit1,lwd=3,col="red")
confint(fit1)
predict(fit1,data.frame(lstat=(c(5,10,15))), interval="confidence")
# Multiple Linear Regression
fit2=lm(medv~lstat+age,data=Boston)
summary(fit2)
fit3=lm(medv~.,data=Boston)
summary(fit3)
par(mfrow=c(2,2))
plot(fit3)
library(car)
fit5 = lm(medv~lstat*age,data=Boston)
summary(fit5)
fit5 = lm(medv~lstat*age,data=Boston)
summary(fit5)
# Non-linear Transformations of the Predictors
fit6=lm(medv~lstat+I(lstat^2))
summary(fit6)
par(mfrow =c(1,1))
plot(medv~lstat)
points(lstat,fitted(fit6), col= "red", pch= 20)
fit7=lm(medv~poly(lstat,4))
points(lstat,fitted(fit7), col= "blue", pch= 20) #it's seems that it's overfitting
summary(fit7)
fix(Carseats)
names(Carseats)
lm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)
summary(lm.fit)
# for chategorical ones you have a table summarizing the chategories
lm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)
summary(lm.fit)
attach(Carseats)
contrasts(ShelveLoc)
library(FNN)
library(MASS)
data(Boston)
set.seed(42)
boston_idx = sample(1:nrow(Boston), size = 250)
View(Boston)
boston_idx
trn_boston = Boston[boston_idx, ]
tst_boston  = Boston[-boston_idx, ]
X_trn_boston = trn_boston["lstat"]
View(trn_boston)
X_tst_boston = tst_boston["lstat"]
y_trn_boston = trn_boston["medv"]
y_tst_boston = tst_boston["medv"]
X_trn_boston_min = min(X_trn_boston)
X_trn_boston_max = max(X_trn_boston)
lstat_grid = data.frame(lstat = seq(X_trn_boston_min, X_trn_boston_max,
by = 0.01))
pred_001 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 1)
pred_005 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 5)
pred_010 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 10)
pred_050 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 50)
pred_100 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 100)
pred_250 = knn.reg(train = X_trn_boston, test = lstat_grid, y = y_trn_boston, k = 250)
View(Boston)
6840/6000
library(ISLR)
Hitters<-Hitters
View(Hitters)
# Multiple Linear Regression Example
fit <- lm(Salary ~ ., data=Hitters)
summary(fit) # show results
# Other useful functions
coefficients(fit) # model coefficients
confint(fit, level=0.95) # CIs for model parameters
summary(fit) # show results
fitted(fit) # predicted values
residuals(fit) # residuals
anova(fit) # anova table
vcov(fit) # covariance matrix for model parameters
influence(fit) # regression diagnostics
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page
plot(fit)
library(DAAG)
install.packages("DAAG")
cv.lm(df=Hitters, fit, m=3) # 3 fold cross-validation
library(DAAG)
cv.lm(df=Hitters, fit, m=3) # 3 fold cross-validation
?cv.lm
cv.lm(data=Hitters, fit, m=5) # 5 fold cross-validation
dfClean<-Hitters[-is.na(Hitters$Salary),]
cv.lm(data=dfClean, fit, m=5) # 5 fold cross-validation
library(ISLR)
library(DAAG)
dfClean<-Hitters[-is.na(Hitters$Salary),]
# Multiple Linear Regression Example
fit <- lm(Salary ~ ., data=Hitters)
View(fit)
summary(fit) # show results
confint(fit, level=0.95) # CIs for model parameters
cv.lm(data=dfClean, fit, m=5) # 5 fold cross-validation
library(ISLR)
library(DAAG)
dfClean<-Hitters[-is.na(Hitters$Salary),]
# Multiple Linear Regression Example
fit <- lm(Salary ~ ., data=dfClean)
summary(fit) # show results
# Other useful functions
coefficients(fit) # model coefficients
confint(fit, level=0.95) # CIs for model parameters
fitted(fit) # predicted values
residuals(fit) # residuals
anova(fit) # anova table
vcov(fit) # covariance matrix for model parameters
influence(fit) # regression diagnostics
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page
plot(fit)
cv.lm(data=dfClean, fit, m=5) # 5 fold cross-validation
View(dfClean)
dfClean<-Hitters[-is.na(Hitters$Salary),]
dim(Hitters)
is.na(Hitters)
colSums(is.na(Hitters))
dfClean<-Hitters[!is.na(Hitters$Salary),]
# Multiple Linear Regression Example
fit <- lm(Salary ~ ., data=dfClean)
cv.lm(data=dfClean, fit, m=5) # 5 fold cross-validation
cv.lm(data=dfClean, fit, m=5) # 5 fold cross-validation
cv.lm(data=dfClean, fit, m3) # 5 fold cross-validation
cv.lm(data=dfClean, fit, m=3) # 5 fold cross-validation
kv<-cv.lm(data=dfClean, fit, m=3) # 5 fold cross-validation
View(kv)
View(kv)
# Load the data
data("swiss")
# Inspect the data
sample_n(swiss, 3)
library(caret)
library(tidyverse)
# Inspect the data
sample_n(swiss, 3)
# Build the model
model <- lm(Fertility ~., data = train.data)
train.data  <- swiss[training.samples, ]
test.data <- swiss[-training.samples, ]
# Split the data into training and test set
set.seed(123)
training.samples <- swiss$Fertility %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- swiss[training.samples, ]
test.data <- swiss[-training.samples, ]
# Build the model
model <- lm(Fertility ~., data = train.data)
# Make predictions and compute the R2, RMSE and MAE
predictions <- model %>% predict(test.data)
data.frame( R2 = R2(predictions, test.data$Fertility),
RMSE = RMSE(predictions, test.data$Fertility),
MAE = MAE(predictions, test.data$Fertility))
# Define training control
train.control <- trainControl(method = "LOOCV")
# Train the model
model <- train(Fertility ~., data = swiss, method = "lm",
trControl = train.control)
# Summarize the results
print(model)
train.control <- trainControl(method = "cv", number = 10)
# Train the model
model <- train(Fertility ~., data = swiss, method = "lm",
trControl = train.control)
# Summarize the results
print(model)
print(model)
summary(model)
confint(model,level = 0.95)
predict(model$finalModel, train.data, interval = "confidence")
library(ISLR)
library(DAAG)
dfClean<-Hitters[!is.na(Hitters$Salary),]
# Multiple Linear Regression Example
fit <- lm(Salary ~ ., data=dfClean)
summary(fit) # show results
# Other useful functions
coefficients(fit) # model coefficients
confint(fit, level=0.95) # CIs for model parameters
fitted(fit) # predicted values
residuals(fit) # residuals
anova(fit) # anova table
data(mtcars)    # Load the dataset
head(mtcars)
# linear regression model
data(mtcars)    # Load the dataset
head(mtcars)
# linear regression model
library(caret)# Simple linear regression model (lm means linear model)
model <- train(mpg ~ wt,
data = mtcars,
method = "lm")
# Multiple linear regression model
model <- train(mpg ~ .,
data = mtcars,
method = "lm")
summary(model)
# Ridge regression model
model <- train(mpg ~ .,
data = mtcars,
method = "ridge") # Try using "lasso"
install.packages("elasticnet")
# Multiple linear regression model
model <- train(mpg ~ .,
data = mtcars,
method = "lm")
summary(model)
# Ridge regression model
model <- train(mpg ~ .,
data = mtcars,
method = "ridge") # Try using "lasso"
print(model)
model
summary(model)
# Multiple linear regression model
model <- train(mpg ~ .,
data = mtcars,
method = "lm")
summary(model)
## 10-fold CV# possible values: boot", "boot632", "cv", "repeatedcv", "LOOCV", "LGOCV"
fitControl <- trainControl(method = "repeatedcv",
number = 10,     # number of folds
repeats = 10)    # repeated ten times
model.cv <- train(mpg ~ .,
data = mtcars,
method = "lasso",  # now we're using the lasso method
trControl = fitControl)
model.cv
model.cv <- train(mpg ~ .,
data = mtcars,
method = "lasso",
trControl = fitControl,
preProcess = c('scale', 'center')) # default: no pre-processing
?train    # if you need more information about the train function
model.cv
# Here I generate a dataframe with a column named lambda with 100 values that goes from 10^10 to 10^-2
lambdaGrid <- expand.grid(lambda = 10^seq(10, -2, length=100))
model.cv <- train(mpg ~ .,
data = mtcars,
method = "ridge",
trControl = fitControl,
preProcess = c('scale', 'center'),
tuneGrid = lambdaGrid,   # Test all the lambda values in the lambdaGrid dataframe
na.action = na.omit)   # Ignore NA values
model.cv
ggplot(varImp(model.cv))
predictions <- predict(model.cv, mtcars)
predictions
data(mtcars)    # Load the dataset
head(mtcars)
# linear regression model
library(caret)# Simple linear regression model (lm means linear model)
data(mtcars)    # Load the dataset
head(mtcars)
# regression over a variable
model <- train(mpg ~ wt,
data = mtcars,
method = "lm")
# Multiple linear regression model
model <- train(mpg ~ .,
data = mtcars,
method = "lm")
summary(model)
# Ridge regression model
model <- train(mpg ~ .,
data = mtcars,
method = "ridge") # Try using "lasso"
summary(model)
View(model)
?train
## 10-fold CV# possible values: boot", "boot632", "cv", "repeatedcv", "LOOCV", "LGOCV"
fitControl <- trainControl(method = "repeatedcv",
number = 10,     # number of folds
repeats = 10)    # repeated ten times
model.cv <- train(mpg ~ .,
data = mtcars,
method = "lasso",  # now we're using the lasso method
trControl = fitControl)
model.cv
View(model.cv)
# we add some preprocessing. We scale the data for the ridge regression
model.cv <- train(mpg ~ .,
data = mtcars,
method = "lasso",
trControl = fitControl,
preProcess = c('scale', 'center')) # default: no pre-processing
model.cv
# Here I generate a dataframe with a column named lambda with 100 values that goes from 10^10 to 10^-2
lambdaGrid <- expand.grid(lambda = 10^seq(10, -2, length=100))
lambdaGrid
model.cv <- train(mpg ~ .,
data = mtcars,
method = "ridge",
trControl = fitControl,
preProcess = c('scale', 'center'),
tuneGrid = lambdaGrid,   # Test all the lambda values in the lambdaGrid dataframe
na.action = na.omit)   # Ignore NA values
model.cv %>% arrange(RMSE)
model.cv
View(model.cv)
ggplot(varImp(model.cv))
View(mtcars)
# Ridge regression model
model <- train(mpg ~ .,
data = mtcars,
method = "ridge") # Try using "lasso"
# Load the data
data("Boston", package = "MASS")
# Split the data into training and test set
set.seed(123)
str(Boston)
?model.matrix
data("Boston", package = "MASS")
str(Boston)
