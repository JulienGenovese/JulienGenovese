{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cecaff5c",
   "metadata": {},
   "source": [
    "# BATCH GRADIENT DESCENT WITH EARLY STOPPING FOR SOFTMAX REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7043095a",
   "metadata": {},
   "source": [
    "In this notebook we will implement a batch gradient descent with early stopping for softmax regression (ex 12 ch. 4 of \"**Hands-On Machine Learning with Scikit-Learn, Keras & Tensorflow**\" by Aurélien Géron3. \n",
    "\n",
    "We have to implement it by scratch without using scikit-learn. \n",
    "\n",
    "We will try to formulate the problem in the more optimized way to obtain speed results and beatiful formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff6ba21",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f46c365",
   "metadata": {},
   "source": [
    "We start import all we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e1a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5eaf67",
   "metadata": {},
   "source": [
    "In this part we introduce the **softmax_score** for an instance $\\textbf{x}$ for class $k$ defined as:\n",
    "$$\n",
    "s_k(\\textbf{x}) = \\textbf{x}^T \\theta^{(k)}\n",
    "$$\n",
    "of dimension $(1,)$\n",
    "so, for a dataframe we obtain:\n",
    "\n",
    "$$\n",
    "s_k(\\textbf{X}) = \\textbf{X} \\theta^{(k)}\n",
    "$$\n",
    "of dimensions $(\\mbox{number of row of X}, )$.\n",
    "\n",
    "Finally for all classes: \n",
    "\n",
    "$$\n",
    "\\textbf{s}(\\textbf{X}) = (s_1(\\textbf{X}), s_2(\\textbf{X}), ..., s_k(\\textbf{X})) =  \\textbf{X} \\Theta = \\textbf{X} \\big[\\theta^1, \\theta^2, ... , \\theta^k\\big]\n",
    "$$\n",
    "of  $dim\\big(\\textbf{s}(\\textbf{X})\\big) = (\\mbox{number of row of X}, \\mbox{numer of classes K})$\n",
    "\n",
    "and $\\textit{dim}(\\Theta)= ({\\mbox{number of columns}, \\mbox{number of classes K}})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e19963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_score(X, Theta):\n",
    "    score = np.matmul(X, Theta)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb74c94",
   "metadata": {},
   "source": [
    "The softmax function for the instance $\\textbf{x}$ is:\n",
    "\n",
    "$$\n",
    "\\hat{p}_k(\\textbf{x}) = \\sigma(\\textbf{s}(\\textbf{x}))_k = \\dfrac{exp(s_k(\\textbf{x}))}{\\sum_{j = 1}^K exp(s_j(\\textbf{x}))}\n",
    "$$\n",
    "of dimension $(1,)$.\n",
    "\n",
    "So, for a dataframe we obtain:\n",
    "\\begin{align} \n",
    "\\hat{P}(\\textbf{X}) = \\begin{bmatrix}\n",
    "     \\dfrac{exp(s_1(\\textbf{x}_1))}{\\sum_{j = 1}^K exp(s_j(\\textbf{x}_1))}, ..., \\dfrac{exp(s_k(\\textbf{x}_1))}{\\sum_{j = 1}^K exp(s_j(\\textbf{x}_1))} \\\\\n",
    "    \\vdots \\\\\n",
    "     \\dfrac{exp(s_1(\\textbf{x}_n))}{\\sum_{j = 1}^K exp(s_j(\\textbf{x}_n))}, ..., \\dfrac{exp(s_k(\\textbf{x}_n))}{\\sum_{j = 1}^K exp(s_j(\\textbf{x}_n))} \\\\\n",
    "     \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "of dimensions $(\\mbox{number of row of X},  \\mbox{numer of classes K})$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751aa3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_function(X, Theta, k=None):\n",
    "    P_not_normalized = np.exp(softmax_score(X, Theta)) \n",
    "    P = P_not_normalized / np.sum(P_not_normalized, axis=1, keepdims=True)\n",
    "    if k is None:\n",
    "        return P\n",
    "    else:\n",
    "        return P[:, k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae5637d",
   "metadata": {},
   "source": [
    "We define a learning schedule to reduce the learning rate parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa4563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_schedule(t):\n",
    "    t_0, t_1 = 5, 50 \n",
    "    return t_0 / (t + t_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c881b644",
   "metadata": {},
   "source": [
    "And finally the batch gradient descent.\n",
    "\n",
    "If we define the target variables in a one hot encoding way, as in \\begin{align} \n",
    "\\textbf{Y} =  \\begin{bmatrix}\n",
    "    y^{(1)} \\\\\n",
    "    y^{(2)} \\\\\n",
    "    \\vdots \\\\\n",
    "     y^{(\\textit{number of row})}  \\\\\n",
    "     \\end{bmatrix} = \\begin{bmatrix}\n",
    "    1, 0, 0, 0 \\\\\n",
    "    0, 1, 0, 0\\\\\n",
    "    \\vdots \\\\\n",
    "     0,0,0,1 \\\\\n",
    "     \\end{bmatrix}\n",
    "\\end{align}\n",
    "we can define the loss as:\n",
    "$$\n",
    "J(\\Theta) = -\\dfrac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^K  y_k^{(i)}log\\big(\\hat{p}_k^{(i)}\\big) =  -\\dfrac{1}{m} \\mbox{trace}(\\textbf{Y}log(\\hat{P})^T) \n",
    "$$\n",
    "and the gradient of the loss:\n",
    "$$\n",
    "\\nabla_{\\theta^k}J(\\Theta)= \\frac{1}{m}\\sum_{i=1}^m \\big(\\hat{p}_k^{(i)} - y_k^{(i)}\\big)\\textbf{x}^{(i)}\n",
    "$$\n",
    "and defining the matrix:\n",
    "$$\n",
    "\\nabla_{\\Theta}J(\\Theta) = \\dfrac{1}{m}\\textbf{X}^T \\big[\\textbf{P}- \\textbf{Y}\\big]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ad5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(Y, P):\n",
    "    return -1/m * np.trace(np.matmul(Y, np.log(P).transpose()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231f396",
   "metadata": {},
   "outputs": [],
   "source": [
    " def batch_gradient_descent(X, y, learning_rate, n_epochs):\n",
    "    \n",
    "    Theta = np.random.randn(X.shape[1], len(np.unique(y)))\n",
    "    m = y.shape[0]\n",
    "    Y = np.zeros((y.size, y.max()+1))\n",
    "    Y[np.arange(y.size),y] = 1\n",
    "    loss = []\n",
    "    eta = 1e-7\n",
    "    for epoch in range(n_epochs):\n",
    "        P =  softmax_function(X, Theta)\n",
    "        loss.append(loss_function(Y,P))\n",
    "        gradient = 1/m * np.dot(X.transpose(), P - Y)\n",
    "        Theta = Theta - learning_rate * gradient\n",
    "        learning_rate = learning_schedule(learning_rate)\n",
    "    return theta, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fefefc1",
   "metadata": {},
   "source": [
    "## CREATION OF THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "f8db5c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_log_reg():\n",
    "    def __init__(self, X, y, learning_rate=None, n_epochs=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def scale(self, X):\n",
    "        mean_X = np.mean(X, axis = 0)\n",
    "        std_X = np.std(X, axis = 0)\n",
    "        X_meanZero = X - mean_X\n",
    "        X_normalized = X_meanZero / std_X\n",
    "        return X_normalized\n",
    "\n",
    "    def softmax_function(self, X, Theta):\n",
    "        def softmax_score(X, Theta):\n",
    "            score = np.matmul(X, Theta)\n",
    "            return score\n",
    "        P_not_normalized = np.exp(softmax_score(X, Theta)) \n",
    "        P = P_not_normalized / np.sum(P_not_normalized, axis=1, keepdims=True)\n",
    "        return P\n",
    "    \n",
    "    def loss_function(self, Y, P):\n",
    "        m = Y.shape[0]\n",
    "        return -1/m * np.trace(np.matmul(Y, np.log(P).transpose()))\n",
    "    \n",
    "    def batch_gradient_descent(self, X, y, learning_rate, n_epochs):\n",
    "            X_scaled = self.scale(X)\n",
    "            X_scaled = np.c_[np.ones([len(X_scaled), 1]), X_scaled]\n",
    "            \n",
    "            Theta = np.random.randn(X_scaled.shape[1], len(np.unique(y)))\n",
    "            m = y.shape[0]\n",
    "            # one-hot-encoding\n",
    "            Y = np.zeros((y.size, y.max()+1))\n",
    "            Y[np.arange(y.size),y] = 1\n",
    "            loss = []\n",
    "            \n",
    "            def learning_schedule(t):\n",
    "                t_0, t_1 = 5, 50 \n",
    "                return t_0 / (t + t_1)\n",
    "            \n",
    "            for iteration in range(n_epochs):\n",
    "                P =  self.softmax_function(X_scaled, Theta)\n",
    "                loss_value = self.loss_function(Y,P)\n",
    "                loss.append(loss_value)\n",
    "                if iteration % 500 == 0:\n",
    "                    print(iteration, loss_value)\n",
    "                gradient = 1/m * np.matmul(X_scaled.transpose(), P - Y)\n",
    "                Theta = Theta - learning_rate * gradient\n",
    "                learning_rate = learning_schedule(learning_rate)\n",
    "            return Theta, loss\n",
    "        \n",
    "    def fit(self, learning_rate=None, n_epochs=None):\n",
    "        if learning_rate is None:\n",
    "            self.learning_rate = 0.1\n",
    "        else:\n",
    "            self.learning_rate = learning_rate\n",
    "        if n_epochs is None:\n",
    "            self.n_epochs = 100\n",
    "        else:\n",
    "            self.n_epochs = n_epochs\n",
    "            \n",
    "        self.Theta, self.loss = self.batch_gradient_descent(self.X, self.y, self.learning_rate, self.n_epochs)   \n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scale(X)\n",
    "        X_scaled = np.c_[np.ones([len(X_scaled), 1]), X_scaled]\n",
    "        P = self.softmax_function(X_scaled, self.Theta)\n",
    "        return np.amax(P, axis=1)\n",
    "    \n",
    "    def plot_loss(self):\n",
    "        n_epocs_vector = np.arange(self.n_epochs)\n",
    "        loss_vector = self.loss\n",
    "        plt.plot(n_epocs_vector, loss_vector)\n",
    "        \n",
    "    def return_Theta(self):\n",
    "        return self.Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "df49009a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.767900645523398\n",
      "500 0.1809292958702448\n",
      "1000 0.13639240671497782\n",
      "1500 0.11786781596804172\n",
      "2000 0.1074136109096289\n",
      "2500 0.10059310880543518\n",
      "3000 0.09574767038675101\n",
      "3500 0.09210729165544891\n",
      "4000 0.08926222763445296\n",
      "4500 0.08697276587810962\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_target = iris[\"target\"]\n",
    "iris_data = iris[\"data\"][:, (2, 3)]\n",
    "\n",
    "reg_model = model_log_reg(iris_data, iris_target)\n",
    "reg_model.fit(learning_rate = 0.01, n_epochs=5000)\n",
    "reg_model.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "44c09258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.65680815,  3.18260765, -3.24328671],\n",
       "       [-3.597217  ,  0.52862985,  5.17525011],\n",
       "       [-3.80669277,  0.01062575,  5.96500872]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_model.return_Theta()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
