{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cecaff5c",
   "metadata": {},
   "source": [
    "# BATCH GRADIENT DESCENT WITH EARLY STOPPING FOR SOFTMAX REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7043095a",
   "metadata": {},
   "source": [
    "In this notebook we will implement a batch gradient descent with early stopping for softmax regression (ex 12 ch. 4 of \"**Hands-On Machine Learning with Scikit-Learn, Keras & Tensorflow**\" by Aurélien Géron3. \n",
    "\n",
    "We have to implement it by scratch without using scikit-learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff6ba21",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f46c365",
   "metadata": {},
   "source": [
    "We start import all we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b1e1a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0fd45221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_score(x, theta):\n",
    "    score = np.dot(x.transpose(), theta)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "46aa4563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_function(x, theta, k=None):\n",
    "    if k is None:\n",
    "        p = np.exp(softmax_score(x, theta)) / np.sum(np.exp(softmax_score(x, theta)))\n",
    "        return p\n",
    "    else:\n",
    "        p_k = np.exp(softmax_score(x, theta[:, k - 1])) / np.sum(np.exp(softmax_score(x, theta)))\n",
    "        return p_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "314125f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.8,0.7,0.2])\n",
    "theta = np.array([[0.2,0.1,0.5], [0.3,0.2,0.1], [0.3,0.4,0.5]])\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3a578880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2, 0.1, 0.5],\n",
       "       [0.3, 0.2, 0.1],\n",
       "       [0.3, 0.4, 0.5]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5ceece88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.7094662 , -0.74635296,  1.41075455],\n",
       "       [ 1.47065076,  1.25936043,  0.71852724],\n",
       "       [ 1.22854529, -0.46854869, -0.47496751]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(theta.shape[0], theta.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1967446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_schedule(t):\n",
    "    t_0, t_1 = 5, 50 \n",
    "    return t_0 / (t + t_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ee1e1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent(y, X, learning_rate = 0.1, n_epochs = 100):\n",
    "    theta = np.random.randn(X.shape[0], X.shape[1])\n",
    "    m = y.shape[0]\n",
    "    for epoch in range(n_epochs):\n",
    "        gradient = 1/m * np.sum(np.dot(softmax_function(y, X) - y, X))\n",
    "        theta = theta - learning_rate * gradient\n",
    "        learning_rate = learning_schedule(learning_rate)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "43f9e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_data = iris[\"data\"]\n",
    "iris_target = iris[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f6eac142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_346/407382810.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  p = np.exp(softmax_score(x, theta)) / np.sum(np.exp(softmax_score(x, theta)))\n",
      "/tmp/ipykernel_346/407382810.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p = np.exp(softmax_score(x, theta)) / np.sum(np.exp(softmax_score(x, theta)))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,) (150,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_346/1789421372.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miris_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miris_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_346/1355636226.py\u001b[0m in \u001b[0;36mbatch_gradient_descent\u001b[0;34m(y, X, learning_rate, n_epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_schedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,) (150,) "
     ]
    }
   ],
   "source": [
    "batch_gradient_descent(iris_target, iris_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
