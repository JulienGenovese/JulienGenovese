{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cecaff5c",
   "metadata": {},
   "source": [
    "# BATCH GRADIENT DESCENT WITH EARLY STOPPING FOR SOFTMAX REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7043095a",
   "metadata": {},
   "source": [
    "In this notebook we will implement a batch gradient descent with early stopping for softmax regression (ex 12 ch. 4 of \"**Hands-On Machine Learning with Scikit-Learn, Keras & Tensorflow**\" by Aurélien Géron3. \n",
    "\n",
    "We have to implement it by scratch without using scikit-learn. \n",
    "\n",
    "We will try to formulate the problem in the more optimized way to obtain speed results and beatiful formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff6ba21",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f46c365",
   "metadata": {},
   "source": [
    "We start import all we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e1a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5eaf67",
   "metadata": {},
   "source": [
    "In this part we introduce the **softmax_score** for an instance $\\textbf{x}$ for class $k$ defined as:\n",
    "$$\n",
    "s_k(\\textbf{x}) = \\textbf{x}^T \\theta^{(k)}\n",
    "$$\n",
    "of dimension $(1,)$\n",
    "so, for a dataframe we obtain:\n",
    "\n",
    "$$\n",
    "s_k(\\textbf{X}) = \\textbf{X} \\theta^{(k)}\n",
    "$$\n",
    "of dimensions $(\\mbox{number of row of X}, )$.\n",
    "\n",
    "Finally for all classes: \n",
    "\n",
    "$$\n",
    "\\textbf{s}(\\textbf{X}) = (s_1(\\textbf{X}), s_2(\\textbf{X}), ..., s_k(\\textbf{X})) =  \\textbf{X} \\Theta = \\textbf{X} \\big[\\theta^1, \\theta^2, ... , \\theta^k\\big]\n",
    "$$\n",
    "of  $dim\\big(\\textbf{s}(\\textbf{X})\\big) = (\\mbox{number of row of X}, \\mbox{numer of classes K})$\n",
    "\n",
    "and $\\textit{dim}(\\Theta)= ({\\mbox{number of columns}, \\mbox{number of classes K}})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e19963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_score(X, Theta):\n",
    "    score = np.matmul(X, Theta)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb74c94",
   "metadata": {},
   "source": [
    "The softmax function for the instance $\\textbf{x}$ is:\n",
    "\n",
    "$$\n",
    "\\hat{p}_k(\\textbf{x}) = \\sigma(\\textbf{s}(\\textbf{x}))_k = \\dfrac{exp(s_k(\\textbf{x}))}{\\sum_{j = 1}^K exp(s_j(\\textbf{x}))}\n",
    "$$\n",
    "of dimension $(1,)$.\n",
    "\n",
    "So, for a dataframe we obtain:\n",
    "\\begin{align} \n",
    "\\hat{P}(\\textbf{X}) = \\begin{bmatrix}\n",
    "     \\dfrac{exp(s_1(\\textbf{x}_1))}{\\sum_{j = 1}^K exp(s_j(\\textbf{x}_1))}, ..., \\dfrac{exp(s_k(\\textbf{x}_1))}{\\sum_{j = 1}^K exp(s_j(\\textbf{x}_1))} \\\\\n",
    "    \\vdots \\\\\n",
    "     \\dfrac{exp(s_1(\\textbf{x}_n))}{\\sum_{j = 1}^K exp(s_j(\\textbf{x}_n))}, ..., \\dfrac{exp(s_k(\\textbf{x}_n))}{\\sum_{j = 1}^K exp(s_j(\\textbf{x}_n))} \\\\\n",
    "     \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "of dimensions $(\\mbox{number of row of X},  \\mbox{numer of classes K})$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751aa3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_function(X, Theta, k=None):\n",
    "    P_not_normalized = np.exp(softmax_score(X, Theta)) \n",
    "    P = P_not_normalized / np.sum(P_not_normalized, axis=1, keepdims=True)\n",
    "    if k is None:\n",
    "        return P\n",
    "    else:\n",
    "        return P[:, k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae5637d",
   "metadata": {},
   "source": [
    "We define a learning schedule to reduce the learning rate parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa4563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_schedule(t):\n",
    "    t_0, t_1 = 5, 50 \n",
    "    return t_0 / (t + t_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c881b644",
   "metadata": {},
   "source": [
    "And finally the batch gradient descent.\n",
    "\n",
    "If we define the target variables in a one hot encoding way, as in \\begin{align} \n",
    "\\textbf{Y} =  \\begin{bmatrix}\n",
    "    y^{(1)} \\\\\n",
    "    y^{(2)} \\\\\n",
    "    \\vdots \\\\\n",
    "     y^{(\\textit{number of row})}  \\\\\n",
    "     \\end{bmatrix} = \\begin{bmatrix}\n",
    "    1, 0, 0, 0 \\\\\n",
    "    0, 1, 0, 0\\\\\n",
    "    \\vdots \\\\\n",
    "     0,0,0,1 \\\\\n",
    "     \\end{bmatrix}\n",
    "\\end{align}\n",
    "we can define the loss as:\n",
    "$$\n",
    "J(\\Theta) = -\\dfrac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^K  y_k^{(i)}log\\big(\\hat{p}_k^{(i)}\\big) =  -\\dfrac{1}{m} \\mbox{trace}(\\textbf{Y}log(\\hat{P})^T) \n",
    "$$\n",
    "and the gradient of the loss:\n",
    "$$\n",
    "\\nabla_{\\theta^k}J(\\Theta)= \\frac{1}{m}\\sum_{i=1}^m \\big(\\hat{p}_k^{(i)} - y_k^{(i)}\\big)\\textbf{x}^{(i)}\n",
    "$$\n",
    "and defining the matrix:\n",
    "$$\n",
    "\\nabla_{\\Theta}J(\\Theta) = \\dfrac{1}{m}\\textbf{X}^T \\big[\\textbf{P}- \\textbf{Y}\\big]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ad5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(Y, P):\n",
    "    return -1/m * np.trace(np.matmul(Y, np.log(P).transpose()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be76e2",
   "metadata": {},
   "outputs": [],
   "source": [
    " def batch_gradient_descent(X, y, learning_rate, n_epochs):\n",
    "    \n",
    "    Theta = np.random.randn(X.shape[1], len(np.unique(y)))\n",
    "    m = y.shape[0]\n",
    "    Y = np.zeros((y.size, y.max()+1))\n",
    "    Y[np.arange(y.size),y] = 1\n",
    "    loss = []\n",
    "    eta = 1e-7\n",
    "    for epoch in range(n_epochs):\n",
    "        P =  softmax_function(X, Theta)\n",
    "        loss.append(loss_function(Y,P))\n",
    "        gradient = 1/m * np.dot(X.transpose(), P - Y)\n",
    "        Theta = Theta - learning_rate * gradient\n",
    "        learning_rate = learning_schedule(learning_rate)\n",
    "    return theta, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38db260c",
   "metadata": {},
   "source": [
    "## CREATION OF THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e6f8e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_log_reg():\n",
    "    def __init__(self, X, y, learning_rate=None, n_epochs=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def scale(self, X):\n",
    "        mean_X = np.mean(X, axis = 0)\n",
    "        std_X = np.std(X, axis = 0)\n",
    "        X_meanZero = X - mean_X\n",
    "        X_normalized = X_meanZero / std_X\n",
    "        return X_normalized\n",
    "\n",
    "    def softmax_function(self, X, Theta):\n",
    "        def softmax_score(X, Theta):\n",
    "            score = np.matmul(X, Theta)\n",
    "            return score\n",
    "        P_not_normalized = np.exp(softmax_score(X, Theta)) \n",
    "        P = P_not_normalized / np.sum(P_not_normalized, axis=1, keepdims=True)\n",
    "        return P\n",
    "    \n",
    "    def loss_function(self, Y, P):\n",
    "        m = Y.shape[0]\n",
    "        return -1/m * np.trace(np.matmul(Y, np.log(P).transpose()))\n",
    "    \n",
    "    def batch_gradient_descent(self, X, y, learning_rate, n_epochs):\n",
    "            X_scaled = self.scale(X)\n",
    "            X_scaled = np.c_[np.ones([len(X_scaled), 1]), X_scaled]\n",
    "            \n",
    "            Theta = np.random.randn(X_scaled.shape[1], len(np.unique(y)))\n",
    "            m = y.shape[0]\n",
    "            # one-hot-encoding\n",
    "            Y = np.zeros((y.size, y.max()+1))\n",
    "            Y[np.arange(y.size),y] = 1\n",
    "            loss = []\n",
    "            \n",
    "            def learning_schedule(t):\n",
    "                t_0, t_1 = 5, 50 \n",
    "                return t_0 / (t + t_1)\n",
    "            \n",
    "            for iteration in range(n_epochs):\n",
    "                P =  self.softmax_function(X_scaled, Theta)\n",
    "                loss_value = self.loss_function(Y,P)\n",
    "                loss.append(loss_value)\n",
    "                if iteration % 500 == 0:\n",
    "                    print(iteration, loss_value)\n",
    "                gradient = 1/m * np.matmul(X_scaled.transpose(), P - Y)\n",
    "                Theta = Theta - learning_rate * gradient\n",
    "                learning_rate = learning_schedule(learning_rate)\n",
    "            return Theta, loss\n",
    "        \n",
    "    def fit(self, learning_rate=None, n_epochs=None):\n",
    "        if learning_rate is None:\n",
    "            self.learning_rate = 0.1\n",
    "        else:\n",
    "            self.learning_rate = learning_rate\n",
    "        if n_epochs is None:\n",
    "            self.n_epochs = 100\n",
    "        else:\n",
    "            self.n_epochs = n_epochs\n",
    "            \n",
    "        self.Theta, self.loss = self.batch_gradient_descent(self.X, self.y, self.learning_rate, self.n_epochs)   \n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scale(X)\n",
    "        X_scaled = np.c_[np.ones([len(X_scaled), 1]), X_scaled]\n",
    "        P = self.softmax_function(X_scaled, self.Theta)\n",
    "        return np.amax(P, axis=1)\n",
    "    \n",
    "    def plot_loss(self):\n",
    "        n_epocs_vector = np.arange(self.n_epochs)\n",
    "        loss_vector = self.loss\n",
    "        plt.plot(n_epocs_vector, loss_vector)\n",
    "        \n",
    "    def return_Theta(self):\n",
    "        return self.Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "df49009a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.2532400600041305\n",
      "500 0.18488307162260645\n",
      "1000 0.13704033133960114\n",
      "1500 0.11783876255837683\n",
      "2000 0.10719464708550358\n",
      "2500 0.10032491212096449\n",
      "3000 0.0954791462674174\n",
      "3500 0.0918562517291604\n",
      "4000 0.0890344384616918\n",
      "4500 0.0867690160960914\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_target = iris[\"target\"]\n",
    "iris_data = iris[\"data\"][:, (2, 3)]\n",
    "\n",
    "reg_model = model_log_reg(iris_data, iris_target)\n",
    "reg_model.fit(learning_rate = 0.01, n_epochs=5000)\n",
    "reg_model.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "18932a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02886832,  3.12764205, -0.44998472],\n",
       "       [-2.40072781,  0.32251333,  2.62257937],\n",
       "       [-2.60725234,  0.08739618,  3.75655987]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_model.return_Theta()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
