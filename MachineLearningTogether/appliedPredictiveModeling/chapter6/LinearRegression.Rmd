---------------------------------------------------------------------------
LINEAR MODELS
---------------------------------------------------------------------------

Let's load the solubility dataset to apply the different methods.
```{r}
library(AppliedPredictiveModeling)
data(solubility)
ls(pattern = "^solT")

```
Create our training dataset: 

```{r}
trainingData <- solTrainXtrans 
trainingData$Solubility <- solTrainY
```

--------------- 
CLASSICAL LINEAR REGRESSION 
--------------- 

Let's compute the classical linear regression 
```{r}
lmFitAllPredictors <- lm(Solubility ~ ., data = trainingData)
summary(lmFitAllPredictors)
```
We can see the importance of each predictor using the stars (associated to the p-value)
We have RMSE = 0.5524 and R2 = 0.9446 but they are computed on the training sample. 
Let's see with a test sample:
```{r}
library(caret)
lmPred1 <- predict(lmFitAllPredictors, solTestXtrans)
lmValues1 <- data.frame(obs = solTestY, pred = lmPred1)
defaultSummary(lmValues1)


```
We can see the RMSE is higher and R2 is smaller.

But we want to use a more solid approach to the error estimation using cross validation

```{r}
ctrl <- trainControl(method = "cv", number = 10)
set.seed(100)
lmFit1 <- train(x = solTrainXtrans,
                y = solTrainY,
                method = "lm", 
                trControl = ctrl
                )
```

Let's see the plot of 'residuals vs predicted' and 'observed vs predicted'

```{r}
xyplot(solTrainY ~ predict(lmFit1),
  type = c("p", "g"),  #plot the points (type ='p') and a background grid     ('g')
  xlab = "Predicted",
  ylab = "Observed"
  )
```

```{r}
xyplot(resid(lmFit1) ~ predict(lmFit1),
       type = c("p", "g"),
       xlab = "Predicted", 
       ylab = "Residuals")
```

It's seems that we have some problems of collinearity from the summary of the regression. So we can try to solve it using the algorithm to remove the collinearity:

```{r}
corThresh <- .9 
tooHigh <- findCorrelation(cor(solTrainXtrans), corThresh)
corrPred <- names(solTrainXtrans)[tooHigh]
trainXfiltered <- solTrainXtrans[, -tooHigh]
testXfiltered <- solTestXtrans[, -tooHigh]
set.seed(100)
lmFiltered <- train(solTrainXtrans, solTrainY, method = "lm",                    trControl = ctrl)
lmFiltered
```
We cannot see big improvements!!
We need some other methods!

------------------------------------------------------------------------
ROBUST LINEAR REGRESSION
-----

We want to move on Robust Linear regression using the Huber Loss:

```{r}
library(MASS)
rlmFitAllPredictors <- rlm(Solubility ~ ., data = trainingData)
summary(rlmFitAllPredictors)
```
We don't have the p-value: this is related with the implementation of the package.

Let's use now some solid test error estimation but we preproc. the data because it's important for the robust linear regression:

```{r}
set.seed(100)
rlmPCA <- train(solTrainXtrans, solTrainY,
  method = "rlm",
  preProcess = "pca",
  trControl = ctrl
) 
rlmPCA
```
It takes some time because I think we are solving an optimization problem  we don't have a closed formula.

------------------------------------------------------------------------
DIMENSIONALITY REDUCTION METHODS
------------------------------------------------------------------------

```{r}
modelPCA <- train(
  Solubility ~ .,
  data = trainingData, 
  method = "pcr",
  scale = TRUE,
  trControl = trainControl("cv", number = 10),
  tuneLength = 228
  )
print(modelPCA)
plot(modelPCA)
```


Let's use different partial least squares methods using the first algorithm
```{r}
library(pls)
plsFit <- plsr(Solubility ~ ., data = trainingData, ncomp = 15)
plot(plsFit)
```
With the option ncomp we ridgeRegFit <-
  train(
  solTrainXtrans,
  solTrainY,
  method = "ridge",
  tuneGrid = ridgeGrid,
  trControl = ctrl,
  preProc = c("center", "scale")
can choose the number of components that we want

```{r}
predict(plsFit, solTestXtrans[1:5,], ncomp = 1:2)
```
This give us the results associated with two components.

We can also use the caret package:
```{r}
set.seed(100)

plsTune <- train(solTrainXtrans, solTrainY,
                 method = "pls",
                 tuneLength = 200,
                 trControl = ctrl,
                 preProc = c("center", "scale")
                 )
print(plsTune)
plot(plsTune)
```

------------------------------------------------------------------------
PENALIZED METHODS
------------------------------------------------------------------------

Let's begin with a ridge regression tuning it's lambda penalization:
```{r}
ridgeGrid <- data.frame(.lambda = seq(0, .1, length = 15))
library(doMC)
registerDoMC(cores = 5)
set.seed(100)
ridgeRegFit <-
  train(
  solTrainXtrans,
  solTrainY,
  method = "ridge",
  tuneGrid = ridgeGrid,
  trControl = ctrl,
  preProc = c("center", "scale")
  )
ridgeRegFit
```

```{r}
plot(ridgeRegFit, xlab ="Penalty")
```


And passing to the lasso:

```{r}
fraction <- seq(0.01, 0.20, length = 100)
set.seed(100)
lassoRegFit <- train(
  solTrainXtrans,
  solTrainY,
  method = "enet",
  tuneGrid = expand.grid(fraction = fraction, lambda = 0),
  trControl = ctrl,
  preProc = c("center", "scale")
  )
lassoRegFit
plot(lassoRegFit)
```

And for the elastic net:

```{r}

enetGrid <- expand.grid(.lambda = c(0, 0.01, .1),
                        .fraction = seq(.05, 1, length = 20)
                        )
set.seed(100)
enetTune <- train(solTrainXtrans, solTrainY,
                  method = "enet",
                  tuneGrid = enetGrid,
                  trControl = ctrl,
                  preProc = c("center", "scale")
                  )
enetTune
```

```{r}
plot(enetTune)
```


------------------------------
Non linear methods: neural network
-----------------------------



```{r}
library(doMC)
registerDoMC(cores = 3)
tooHigh <- findCorrelation(cor(solTrainXtrans), cutoff = .75)
trainXnnet <- solTrainXtrans[,-tooHigh]
testXnnet <- solTestXtrans[,-tooHigh]
nnetGrid <- expand.grid(.decay = c(0.01, .1), # regularization parameter
                        .size = c(1:10), # size for gradient descent
                        .bag = FALSE) # use bagging for resampling

ctrl <- trainControl(method = "cv", number = 10)

set.seed(100)
nnetTune <- train(
  solTrainXtrans,
  solTrainY,
  method = "avNNet",
  tuneGrid = nnetGrid,
  trControl = ctrl,
  preProc = c("center", "scale"), # center and scale 
  linout = TRUE, # we want a regression model
  trace = FALSE, # less output printed
  MaxNWts = 6 * (ncol(trainXnnet) + 1) + 6 + 1, # 6 hidden layers
  maxit = 500
)
print(nnetTune)
plot(nnetTune)
```

